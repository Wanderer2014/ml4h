{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to ML4D BroadE!\n",
    "\n",
    "### Agenda\n",
    "10:00-10:05: Intro Videos, Demos and Downloads \n",
    "\n",
    "10:05-10:30: Run docker jupyter server and open broad.ipynb and run first cell\n",
    "\n",
    "10:30-10:45: Fundamental ML (linear and logistic regression, SGD, depth and interpretation)\n",
    "\n",
    "10:45-11:00: Train models on well-studied datasets (MNIST, CIFAR10)\n",
    "\n",
    "11:00-11:30: ML4CVD Abstractions: Tensorization, TensorMaps and the ModelFactory\n",
    "\n",
    "11:30-11:45: Coffee Break, Discussion and Data Exploration (groups coalesce: feature not bug)\n",
    "\n",
    "11:45-11:55: Neurologist pep Talk from Chris Anderson\n",
    "\n",
    "11:55-12:30: Defining TensorMaps on Qure.ai sets.\n",
    "\n",
    "12:15-12:30: Training models on the datasets\n",
    "\n",
    "12:30-12:45: Saliency Maps\n",
    "\n",
    "12:45-12:59: Discussion and TensorMaps on your data, Tensorboard, Hyperparameter optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Basic comfort with python, some linear algebra, some data science\n",
    "- Download & Install Docker: https://docs.docker.com/install/\n",
    "- Download ML4D_BroadE.zip from Google Drive: \n",
    "- Unzip ml-broade.zip\n",
    "- Open a Command line terminal and `cd` to the ml-broade directory\n",
    "- Build your docker! run `docker build -t ml4cvd-mkl .` (This will take a little while)\n",
    "- One docker is built run the jupyter server: `docker run -it -p 8888:8888 -u $(id -u):$(id -g) --cpus=4 --memory=8g -v $PWD:/tf -it ml4cvd-mkl:latest`\n",
    "- Download ml-broade-data folder (big or small depending on how much free space you have).\n",
    "- Once data is downloaded, kill the jupyter server hold `ctrl-c` in the terminal and restart with: `docker run -it -v $HOME/ml-broade-data/:/data -p 6006:6006 -p 8888:8888 -u $(id -u):$(id -g) --cpus=4 --memory=8g -v $PWD:/tf -it ml4cvd-mkl:latest` (edit the path `$HOME/ml-broade-data/` to point at your data folder download location.)\n",
    "- Click the trust button on your Jupyter notebook\n",
    "- Now we are ready to teach the machines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Dict, Callable\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import csv\n",
    "import gzip\n",
    "import h5py\n",
    "import shutil\n",
    "import zipfile\n",
    "import pydicom\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "from ml4cvd.defines import StorageType\n",
    "from ml4cvd.arguments import parse_args, TMAPS, _get_tmap\n",
    "from ml4cvd.TensorMap import TensorMap, Interpretation\n",
    "from ml4cvd.tensor_generators import test_train_valid_tensor_generators\n",
    "from ml4cvd.models import train_model_from_generators, make_multimodal_multitask_model, _inspect_model\n",
    "from ml4cvd.recipes import test_multimodal_multitask, train_multimodal_multitask, saliency_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ZIP_FOLDER = '/data/zips/' \n",
    "HD5_FOLDER = '/data/hd5s/'\n",
    "MODEL_FOLDER = './models/'\n",
    "EXCLUDE_SERIES = ['4cc', '_&_', '5mm', '3mm', 'helical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python features we make lots of use of in this notebook:\n",
    "- F Strings\n",
    "- Callback Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "We explore machine learning on Bio medical data using Cloud computing, Python, Tensorflow, and the ML4CVD codebase.\n",
    "\n",
    "We will start with linear regression.  Our model is a vector, one weight for each input feature, and a single bias weight.\n",
    "\n",
    "\\begin{equation}\n",
    "y = xw + b\n",
    "\\end{equation}\n",
    "\n",
    "For notational convenience absorb the bias term into the weight vector by adding a 1 to the input data matrix $X$\n",
    "\n",
    "\\begin{equation}\n",
    "y = [\\textbf{1}, X][b, \\textbf{w}]^T\n",
    "\\end{equation}\n",
    "\n",
    "#### The Dense Layer is Matrix (Tensor) Multiplication\n",
    "![Matrix Multiplication](https://www.mathwarehouse.com/algebra/matrix/images/matrix-multiplication/how-to-multiply-2-matrices-demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "    samples = 40\n",
    "    real_weight = 2.0\n",
    "    real_bias = 0.5\n",
    "    x = np.linspace(-1, 1, samples)\n",
    "    y = real_weight*x + real_bias + (np.random.randn(*x.shape) * 0.1)\n",
    "\n",
    "    linear_model = Sequential()\n",
    "    linear_model.add(Dense(1, input_dim=1))\n",
    "    linear_model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    linear_model.summary()\n",
    "    linear_model.fit(x, y, batch_size=1, epochs=6)\n",
    "\n",
    "    learned_slope = linear_model.get_weights()[0][0][0]\n",
    "    learned_bias = linear_model.get_weights()[1][0]\n",
    "    print('Learned slope:',  learned_slope, 'real slope:', real_weight, 'learned bias:', learned_bias, 'real bias:', real_bias)\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.plot([-1,1], [-learned_slope+learned_bias, learned_slope+learned_bias], 'r')\n",
    "    plt.show()\n",
    "    print('Linear Regression complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Logistic Regression:\n",
    "We take the real-valued predictions from linear regression and squish them with a sigmoid.\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y} = \\sigma(X\\textbf{w} + b)\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(np.exp(item)/(1+np.exp(item)))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "plt.plot(x,sig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(epochs = 600, num_labels = 10):\n",
    "    train, test, valid = load_data('mnist.pkl.gz')  \n",
    "    \n",
    "    train_y = make_one_hot(train[1], num_labels)\n",
    "    valid_y = make_one_hot(valid[1], num_labels)\n",
    "    test_y = make_one_hot(test[1], num_labels)\n",
    "\n",
    "    logistic_model = Sequential()\n",
    "    logistic_model.add(Dense(num_labels, activation='softmax', input_dim=784, name='mnist_templates'))\n",
    "    logistic_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    logistic_model.summary()\n",
    "    \n",
    "    templates = logistic_model.layers[0].get_weights()[0]\n",
    "    plot_templates(templates, 0)\n",
    "    print('weights shape:', templates.shape)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        trainidx = random.sample(range(0, train[0].shape[0]), 8192)\n",
    "        x_batch = train[0][trainidx,:]\n",
    "        y_batch = train_y[trainidx]\n",
    "        logistic_model.train_on_batch(x_batch, y_batch)\n",
    "        if e % 100 == 0:\n",
    "            plot_templates(logistic_model.layers[0].get_weights()[0], e)\n",
    "            print('Logistic Model test set loss and accuracy:', logistic_model.evaluate(test[0], test_y), 'at epoch', e)\n",
    "\n",
    "\n",
    "def plot_templates(templates, epoch):\n",
    "    n = 10\n",
    "    templates = templates.reshape((28,28,n))\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, 5, i+1)\t\t\n",
    "        plt.imshow(templates[:, :, i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plot_name = \"./regression_example/mnist_templates_\"+str(epoch)+\".png\"\n",
    "    if not os.path.exists(os.path.dirname(plot_name)):\n",
    "        os.makedirs(os.path.dirname(plot_name))\t\t\n",
    "    plt.savefig(plot_name)\n",
    "\n",
    "\n",
    "def make_one_hot(y, num_labels):\n",
    "    ohy = np.zeros((len(y), num_labels))\n",
    "    for i in range(0, len(y)):\n",
    "        ohy[i][y[i]] = 1.0\n",
    "    return ohy\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    ''' Loads the dataset\n",
    "    :param dataset: the path to the dataset (here MNIST)'''\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\"data\", dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from urllib.request import urlretrieve\n",
    "        origin = ('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "        print('Downloading data from %s' % origin)\n",
    "        if not os.path.exists(os.path.dirname(dataset)):\n",
    "            os.makedirs(os.path.dirname(dataset))\t\n",
    "        urlretrieve(origin, dataset)\n",
    "\n",
    "    print('loading data...')\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    if sys.version_info[0] == 3:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        train_set, valid_set, test_set = u.load()\n",
    "    else:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def plot_mnist(sides):\n",
    "    train, _, _ = load_data('mnist.pkl.gz')\n",
    "    print(train[0].shape)\n",
    "    mnist_images = train[0].reshape((-1, 28, 28, 1))\n",
    "    sides = int(np.ceil(np.sqrt(min(sides, mnist_images.shape[0]))))\n",
    "    _, axes = plt.subplots(sides, sides, figsize=(16, 16))\n",
    "    for i in range(sides*sides):\n",
    "        axes[i // sides, i % sides].imshow(mnist_images[i, ..., 0], cmap='gray')\n",
    "\n",
    "def mnist_as_hd5(hd5_folder):\n",
    "    train, _, _ = load_data('mnist.pkl.gz')\n",
    "    print(f' train 0 shape: {train[0].shape} train 1 shape {train[1].shape}')\n",
    "    mnist_images = train[0].reshape((-1, 28, 28, 1))\n",
    "    if not os.path.exists(hd5_folder):\n",
    "        os.makedirs(hd5_folder)\n",
    "    for i, mnist_image in enumerate(mnist_images):\n",
    "        with h5py.File(os.path.join(hd5_folder, f'{i}.hd5'), 'w') as hd5:\n",
    "            hd5.create_dataset('mnist_image', data=mnist_image)\n",
    "            hd5.create_dataset('mnist_label', data=[train[1][i]])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look B4 U Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      " train 0 shape: (50000, 784) train 1 shape (50000,)\n"
     ]
    }
   ],
   "source": [
    "mnist_as_hd5('./mnist_hd5s/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_image_from_hd5(tm, hd5, dependents={}):\n",
    "     return np.array(hd5['mnist_image'])\n",
    "\n",
    "def mnist_label_from_hd5(tm, hd5, dependents={}):\n",
    "    one_hot = np.zeros(tm.shape, dtype=np.float32)\n",
    "    one_hot[int(hd5['mnist_label'][0])] = 1.0\n",
    "    return one_hot\n",
    "    \n",
    "TMAPS['mnist_image'] = TensorMap('mnist_image', shape=(28, 28, 1), tensor_from_file=mnist_image_from_hd5)\n",
    "TMAPS['mnist_label'] = TensorMap('mnist_label', Interpretation.CATEGORICAL, tensor_from_file=mnist_label_from_hd5,\n",
    "                                 channel_map={f'digit_{i}': i for i in range(10)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 16:07:00,021 - logger:25 - INFO - Logging configuration was loaded. Log messages can be found at ./recipes_output/learn_mnist/log_2020-05-20_16-07_0.log.\n",
      "2020-05-20 16:07:00,022 - arguments:372 - INFO - Command Line was: \n",
      "./scripts/tf.sh train --tensors ./mnist_hd5s/ --input_tensors mnist_image --output_tensors mnist_label --test_steps 300 --epochs 4 --id learn_mnist\n",
      "\n",
      "2020-05-20 16:07:00,023 - arguments:373 - INFO - Total TensorMaps: 558 Arguments are Namespace(activation='relu', aligned_dimension=16, alpha=0.5, anneal_max=2.0, anneal_rate=0.0, anneal_shift=0.0, app_csv=None, b_slice_force=None, balance_csvs=[], batch_size=16, bigquery_credentials_file='/mnt/ml4cvd/projects/jamesp/bigquery/bigquery-viewer-credentials.json', bigquery_dataset='broad-ml4cvd.ukbb7089_r10data', block_size=3, bottleneck_type=<BottleneckType.FlattenRestructure: 1>, cache_size=875000000.0, categorical_field_ids=[], continuous_field_ids=[], continuous_file=None, continuous_file_column=None, continuous_file_discretization_bounds=[], continuous_file_normalize=False, conv_dilate=False, conv_dropout=0.0, conv_layers=[32], conv_normalize=None, conv_regularize=None, conv_type='conv', conv_x=3, conv_y=3, conv_z=2, debug=False, dense_blocks=[32, 24, 16], dense_layers=[16, 64], dicom_series='cine_segmented_sax_b6', dicoms='./dicoms/', dropout=0.0, eager=False, epochs=4, freeze_model_layers=False, hidden_layer='embed', id='learn_mnist', imputation_method_for_continuous_fields='random', include_array=False, include_instance=False, include_missing_continuous_channel=False, input_tensors=['mnist_image'], inspect_model=False, inspect_show_labels=True, join_tensors=['partners_ecg_patientid_clean'], label_weights=None, language_layer='ecg_rest_text', language_prefix=None, learning_rate=0.0002, learning_rate_schedule=None, logging_level='INFO', max_models=16, max_parameters=9000000, max_patients=999999, max_pools=[], max_sample_id=7000000, max_samples=None, max_slices=999999, min_sample_id=0, min_samples=3, min_values=10, mixup_alpha=0, mlp_concat=False, mode='mlp', model_file=None, model_files=[], model_layers=None, mri_field_ids=['20208', '20209'], num_workers=4, optimizer='radam', output_folder='./recipes_output/', output_tensors=['mnist_label'], padding='same', patience=8, phecode_definitions='/mnt/ml4cvd/projects/jamesp/data/phecode_definitions1.2.csv', phenos_folder='gs://ml4cvd/phenotypes/', plot_mode='clinical', pool_type='max', pool_x=2, pool_y=2, pool_z=1, random_seed=12878, reference_end_time_tensor=None, reference_join_tensors=None, reference_label=None, reference_name='Reference', reference_start_time_tensor=None, reference_tensors=None, res_layers=[], sample_csv=None, sample_weight=None, t=48, tensor_maps_in=[TensorMap(mnist_image, (28, 28, 1), continuous)], tensor_maps_out=[TensorMap(mnist_label, (10,), categorical)], tensors='./mnist_hd5s/', tensors_name='Tensors', test_csv=None, test_ratio=0.1, test_steps=300, time_tensor='partners_ecg_datetime', train_csv=None, training_steps=400, tsv_style='standard', u_connect=defaultdict(<class 'set'>, {}), valid_csv=None, valid_ratio=0.2, validation_steps=40, write_pngs=False, x=256, xml_field_ids=['20205', '6025'], xml_folder='/mnt/disks/ecg-rest-xml/', y=256, z=48, zip_folder='/mnt/disks/sax-mri-zip/', zoom_height=96, zoom_width=96, zoom_x=50, zoom_y=35)\n",
      "2020-05-20 16:07:03,287 - tensor_generators:602 - INFO - Found 34927 train, 10085 validation, and 4988 testing tensors at: ./mnist_hd5s/\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_mnist_image_continuous (I [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 32)   320         input_mnist_image_continuous[0][0\n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 28, 28, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 14, 14, 32)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 32)   9248        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 14, 14, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 14, 14, 64)   0           max_pooling2d[0][0]              \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 14, 14, 32)   18464       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 14, 14, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 14, 96)   0           max_pooling2d[0][0]              \n",
      "                                                                 activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 32)   27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 14, 14, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 7, 7, 24)     6936        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 7, 7, 24)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 7, 7, 56)     0           max_pooling2d_1[0][0]            \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 7, 7, 24)     12120       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7, 7, 24)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7, 7, 80)     0           max_pooling2d_1[0][0]            \n",
      "                                                                 activation_4[0][0]               \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 24)     17304       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 7, 7, 24)     0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 3, 3, 24)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 3, 3, 16)     3472        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 3, 3, 16)     0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3, 3, 40)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 3, 3, 16)     5776        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 3, 3, 16)     0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 3, 3, 56)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 3, 3, 16)     8080        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 3, 3, 16)     0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 144)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2320        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embed (Dense)                   (None, 64)           1088        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64)           0           embed[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_mnist_label_categorical  (None, 10)           650         activation_11[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 113,458\n",
      "Trainable params: 113,458\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 16:07:04,233 - tensor_generators:149 - INFO - Started 3 train workers with cache size 0.875GB.\n",
      "2020-05-20 16:07:04,397 - tensor_generators:149 - INFO - Started 1 validation workers with cache size 0.875GB.\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/4\n",
      "398/400 [============================>.] - ETA: 0s - loss: 1.9148 - categorical_accuracy: 0.3866 - digit_0_precision: 0.2561 - digit_1_precision: 0.2845 - digit_2_precision: 0.0754 - digit_3_precision: 0.0050 - digit_4_precision: 0.1256 - digit_5_precision: 0.0000e+00 - digit_6_precision: 0.1432 - digit_7_precision: 0.1367 - digit_8_precision: 0.0440 - digit_9_precision: 0.0239 - digit_0_recall: 0.2334 - digit_1_recall: 0.2814 - digit_2_recall: 0.0529 - digit_3_recall: 0.0021 - digit_4_recall: 0.1082 - digit_5_recall: 0.0000e+00 - digit_6_recall: 0.1214 - digit_7_recall: 0.1093 - digit_8_recall: 0.0319 - digit_9_recall: 0.0173      \n",
      "Epoch 00001: val_loss improved from inf to 1.09424, saving model to ./recipes_output/learn_mnist/learn_mnist.h5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 1.9089 - categorical_accuracy: 0.3883 - digit_0_precision: 0.2573 - digit_1_precision: 0.2881 - digit_2_precision: 0.0750 - digit_3_precision: 0.0050 - digit_4_precision: 0.1296 - digit_5_precision: 0.0000e+00 - digit_6_precision: 0.1475 - digit_7_precision: 0.1385 - digit_8_precision: 0.0462 - digit_9_precision: 0.0237 - digit_0_recall: 0.2347 - digit_1_recall: 0.2850 - digit_2_recall: 0.0527 - digit_3_recall: 0.0021 - digit_4_recall: 0.1122 - digit_5_recall: 0.0000e+00 - digit_6_recall: 0.1258 - digit_7_recall: 0.1100 - digit_8_recall: 0.0342 - digit_9_recall: 0.0172 - val_loss: 1.0942 - val_categorical_accuracy: 0.6109 - val_digit_0_precision: 0.7792 - val_digit_1_precision: 0.7000 - val_digit_2_precision: 0.5729 - val_digit_3_precision: 0.2750 - val_digit_4_precision: 0.6375 - val_digit_5_precision: 0.0000e+00 - val_digit_6_precision: 0.5500 - val_digit_7_precision: 0.4938 - val_digit_8_precision: 0.2625 - val_digit_9_precision: 0.0000e+00 - val_digit_0_recall: 0.7521 - val_digit_1_recall: 0.9250 - val_digit_2_recall: 0.4208 - val_digit_3_recall: 0.1625 - val_digit_4_recall: 0.7479 - val_digit_5_recall: 0.0000e+00 - val_digit_6_recall: 0.4625 - val_digit_7_recall: 0.4025 - val_digit_8_recall: 0.1362 - val_digit_9_recall: 0.0000e+00\n",
      "Epoch 2/4\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.5151 - categorical_accuracy: 0.8459 - digit_0_precision: 0.7341 - digit_1_precision: 0.7936 - digit_2_precision: 0.6520 - digit_3_precision: 0.6645 - digit_4_precision: 0.6848 - digit_5_precision: 0.5225 - digit_6_precision: 0.7575 - digit_7_precision: 0.7155 - digit_8_precision: 0.6072 - digit_9_precision: 0.5910 - digit_0_recall: 0.7312 - digit_1_recall: 0.7817 - digit_2_recall: 0.5869 - digit_3_recall: 0.6098 - digit_4_recall: 0.6544 - digit_5_recall: 0.4611 - digit_6_recall: 0.7407 - digit_7_recall: 0.6685 - digit_8_recall: 0.5495 - digit_9_recall: 0.5413\n",
      "Epoch 00002: val_loss improved from 1.09424 to 0.43466, saving model to ./recipes_output/learn_mnist/learn_mnist.h5\n",
      "400/400 [==============================] - 13s 32ms/step - loss: 0.5152 - categorical_accuracy: 0.8458 - digit_0_precision: 0.7354 - digit_1_precision: 0.7921 - digit_2_precision: 0.6488 - digit_3_precision: 0.6662 - digit_4_precision: 0.6842 - digit_5_precision: 0.5236 - digit_6_precision: 0.7579 - digit_7_precision: 0.7144 - digit_8_precision: 0.6092 - digit_9_precision: 0.5880 - digit_0_recall: 0.7325 - digit_1_recall: 0.7803 - digit_2_recall: 0.5839 - digit_3_recall: 0.6111 - digit_4_recall: 0.6561 - digit_5_recall: 0.4638 - digit_6_recall: 0.7420 - digit_7_recall: 0.6664 - digit_8_recall: 0.5509 - digit_9_recall: 0.5386 - val_loss: 0.4347 - val_categorical_accuracy: 0.8734 - val_digit_0_precision: 0.7771 - val_digit_1_precision: 0.7500 - val_digit_2_precision: 0.7104 - val_digit_3_precision: 0.6271 - val_digit_4_precision: 0.6646 - val_digit_5_precision: 0.6625 - val_digit_6_precision: 0.6875 - val_digit_7_precision: 0.7417 - val_digit_8_precision: 0.5542 - val_digit_9_precision: 0.6729 - val_digit_0_recall: 0.7937 - val_digit_1_recall: 0.6845 - val_digit_2_recall: 0.5979 - val_digit_3_recall: 0.6146 - val_digit_4_recall: 0.7292 - val_digit_5_recall: 0.6083 - val_digit_6_recall: 0.8750 - val_digit_7_recall: 0.7021 - val_digit_8_recall: 0.4875 - val_digit_9_recall: 0.5750\n",
      "Epoch 3/4\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.3049 - categorical_accuracy: 0.9091 - digit_0_precision: 0.7859 - digit_1_precision: 0.7909 - digit_2_precision: 0.7387 - digit_3_precision: 0.7252 - digit_4_precision: 0.7379 - digit_5_precision: 0.7076 - digit_6_precision: 0.7353 - digit_7_precision: 0.7688 - digit_8_precision: 0.7563 - digit_9_precision: 0.7371 - digit_0_recall: 0.7775 - digit_1_recall: 0.7865 - digit_2_recall: 0.7028 - digit_3_recall: 0.6898 - digit_4_recall: 0.7262 - digit_5_recall: 0.6911 - digit_6_recall: 0.7213 - digit_7_recall: 0.7414 - digit_8_recall: 0.7028 - digit_9_recall: 0.7185\n",
      "Epoch 00003: val_loss improved from 0.43466 to 0.26287, saving model to ./recipes_output/learn_mnist/learn_mnist.h5\n",
      "400/400 [==============================] - 12s 30ms/step - loss: 0.3045 - categorical_accuracy: 0.9092 - digit_0_precision: 0.7840 - digit_1_precision: 0.7889 - digit_2_precision: 0.7368 - digit_3_precision: 0.7259 - digit_4_precision: 0.7386 - digit_5_precision: 0.7083 - digit_6_precision: 0.7360 - digit_7_precision: 0.7668 - digit_8_precision: 0.7569 - digit_9_precision: 0.7352 - digit_0_recall: 0.7755 - digit_1_recall: 0.7846 - digit_2_recall: 0.7011 - digit_3_recall: 0.6906 - digit_4_recall: 0.7269 - digit_5_recall: 0.6919 - digit_6_recall: 0.7220 - digit_7_recall: 0.7395 - digit_8_recall: 0.7028 - digit_9_recall: 0.7167 - val_loss: 0.2629 - val_categorical_accuracy: 0.9266 - val_digit_0_precision: 0.8250 - val_digit_1_precision: 0.9167 - val_digit_2_precision: 0.8167 - val_digit_3_precision: 0.7250 - val_digit_4_precision: 0.8000 - val_digit_5_precision: 0.7167 - val_digit_6_precision: 0.7229 - val_digit_7_precision: 0.7750 - val_digit_8_precision: 0.7625 - val_digit_9_precision: 0.6471 - val_digit_0_recall: 0.7833 - val_digit_1_recall: 0.8958 - val_digit_2_recall: 0.8104 - val_digit_3_recall: 0.6762 - val_digit_4_recall: 0.7750 - val_digit_5_recall: 0.6583 - val_digit_6_recall: 0.7417 - val_digit_7_recall: 0.6875 - val_digit_8_recall: 0.7292 - val_digit_9_recall: 0.7396\n",
      "Epoch 4/4\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.2346 - categorical_accuracy: 0.9287 - digit_0_precision: 0.7708 - digit_1_precision: 0.8172 - digit_2_precision: 0.7584 - digit_3_precision: 0.8188 - digit_4_precision: 0.7680 - digit_5_precision: 0.7242 - digit_6_precision: 0.7451 - digit_7_precision: 0.7399 - digit_8_precision: 0.6924 - digit_9_precision: 0.7810 - digit_0_recall: 0.7723 - digit_1_recall: 0.8115 - digit_2_recall: 0.7487 - digit_3_recall: 0.8029 - digit_4_recall: 0.7619 - digit_5_recall: 0.7088 - digit_6_recall: 0.7416 - digit_7_recall: 0.7171 - digit_8_recall: 0.6815 - digit_9_recall: 0.7636\n",
      "Epoch 00004: val_loss improved from 0.26287 to 0.19835, saving model to ./recipes_output/learn_mnist/learn_mnist.h5\n",
      "400/400 [==============================] - 12s 31ms/step - loss: 0.2342 - categorical_accuracy: 0.9289 - digit_0_precision: 0.7714 - digit_1_precision: 0.8176 - digit_2_precision: 0.7565 - digit_3_precision: 0.8192 - digit_4_precision: 0.7686 - digit_5_precision: 0.7249 - digit_6_precision: 0.7458 - digit_7_precision: 0.7405 - digit_8_precision: 0.6932 - digit_9_precision: 0.7815 - digit_0_recall: 0.7728 - digit_1_recall: 0.8120 - digit_2_recall: 0.7469 - digit_3_recall: 0.8034 - digit_4_recall: 0.7625 - digit_5_recall: 0.7095 - digit_6_recall: 0.7423 - digit_7_recall: 0.7178 - digit_8_recall: 0.6823 - digit_9_recall: 0.7642 - val_loss: 0.1984 - val_categorical_accuracy: 0.9422 - val_digit_0_precision: 0.7750 - val_digit_1_precision: 0.8917 - val_digit_2_precision: 0.8042 - val_digit_3_precision: 0.6417 - val_digit_4_precision: 0.7417 - val_digit_5_precision: 0.8167 - val_digit_6_precision: 0.8562 - val_digit_7_precision: 0.8333 - val_digit_8_precision: 0.7479 - val_digit_9_precision: 0.8208 - val_digit_0_recall: 0.7208 - val_digit_1_recall: 0.8938 - val_digit_2_recall: 0.8479 - val_digit_3_recall: 0.6750 - val_digit_4_recall: 0.7437 - val_digit_5_recall: 0.7208 - val_digit_6_recall: 0.8583 - val_digit_7_recall: 0.8271 - val_digit_8_recall: 0.7563 - val_digit_9_recall: 0.8146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 16:08:02,276 - tensor_generators:213 - INFO - Stopped 4 workers.\n",
      "2020-05-20 16:08:02,300 - tensor_generators:213 - INFO - Stopped 2 workers.\n",
      "2020-05-20 16:08:02,302 - models:1086 - INFO - Model weights saved at: ./recipes_output/learn_mnist/learn_mnist.h5\n",
      "2020-05-20 16:08:08,194 - plots:203 - INFO - Saved learning curves at:./recipes_output/learn_mnist/metric_history_learn_mnist.png\n",
      "2020-05-20 16:08:08,333 - tensor_generators:149 - INFO - Started 3 test workers with cache size 0.0GB.\n",
      "2020-05-20 16:08:11,002 - tensor_generators:403 - WARNING - Test worker test_worker_0 completed a full epoch. Test results may be double counting samples.\n",
      "2020-05-20 16:08:11,026 - tensor_generators:472 - INFO - Made a big batch of tensors with key:input_mnist_image_continuous and shape:(4800, 28, 28, 1).\n",
      "2020-05-20 16:08:11,028 - tensor_generators:472 - INFO - Made a big batch of tensors with key:output_mnist_label_categorical and shape:(4800, 10).\n",
      "2020-05-20 16:08:11,060 - tensor_generators:403 - WARNING - Test worker test_worker_1 completed a full epoch. Test results may be double counting samples.\n",
      "2020-05-20 16:08:11,122 - tensor_generators:403 - WARNING - Test worker test_worker_2 completed a full epoch. Test results may be double counting samples.\n",
      "2020-05-20 16:08:12,504 - plots:98 - INFO - For tm:mnist_label with channel map:{'digit_0': 0, 'digit_1': 1, 'digit_2': 2, 'digit_3': 3, 'digit_4': 4, 'digit_5': 5, 'digit_6': 6, 'digit_7': 7, 'digit_8': 8, 'digit_9': 9} examples:4800\n",
      "2020-05-20 16:08:12,508 - plots:99 - INFO - \n",
      "Sum Truth:[459. 555. 523. 471. 462. 447. 472. 495. 467. 449.] \n",
      "Sum pred :[415.6846  547.3415  507.83295 477.6084  459.3982  398.0854  486.4198\n",
      " 492.2499  517.47424 497.88852]\n",
      "2020-05-20 16:08:12,535 - plots:1560 - INFO - prAUC Label digit_0 mean precision:0.995 n=459\n",
      "2020-05-20 16:08:12,540 - plots:1560 - INFO - prAUC Label digit_1 mean precision:0.994 n=555\n",
      "2020-05-20 16:08:12,545 - plots:1560 - INFO - prAUC Label digit_2 mean precision:0.981 n=523\n",
      "2020-05-20 16:08:12,551 - plots:1560 - INFO - prAUC Label digit_3 mean precision:0.982 n=471\n",
      "2020-05-20 16:08:12,557 - plots:1560 - INFO - prAUC Label digit_4 mean precision:0.992 n=462\n",
      "2020-05-20 16:08:12,564 - plots:1560 - INFO - prAUC Label digit_5 mean precision:0.993 n=447\n",
      "2020-05-20 16:08:12,569 - plots:1560 - INFO - prAUC Label digit_6 mean precision:0.996 n=472\n",
      "2020-05-20 16:08:12,575 - plots:1560 - INFO - prAUC Label digit_7 mean precision:0.984 n=495\n",
      "2020-05-20 16:08:12,580 - plots:1560 - INFO - prAUC Label digit_8 mean precision:0.978 n=467\n",
      "2020-05-20 16:08:12,586 - plots:1560 - INFO - prAUC Label digit_9 mean precision:0.980 n=449\n",
      "2020-05-20 16:08:12,860 - plots:1575 - INFO - Saved Precision Recall curve at: ./recipes_output/learn_mnist/precision_recall_mnist_label.png\n",
      "2020-05-20 16:08:13,106 - plots:260 - INFO - Try to save calibrations plot at: ./recipes_output/learn_mnist/calibrations_mnist_label.png\n",
      "2020-05-20 16:08:13,537 - plots:1415 - INFO - ROC Label digit_0 area: 0.999 n=459\n",
      "2020-05-20 16:08:13,539 - plots:1415 - INFO - ROC Label digit_1 area: 0.999 n=555\n",
      "2020-05-20 16:08:13,542 - plots:1415 - INFO - ROC Label digit_2 area: 0.997 n=523\n",
      "2020-05-20 16:08:13,544 - plots:1415 - INFO - ROC Label digit_3 area: 0.997 n=471\n",
      "2020-05-20 16:08:13,546 - plots:1415 - INFO - ROC Label digit_4 area: 0.999 n=462\n",
      "2020-05-20 16:08:13,549 - plots:1415 - INFO - ROC Label digit_5 area: 0.999 n=447\n",
      "2020-05-20 16:08:13,551 - plots:1415 - INFO - ROC Label digit_6 area: 1.000 n=472\n",
      "2020-05-20 16:08:13,553 - plots:1415 - INFO - ROC Label digit_7 area: 0.997 n=495\n",
      "2020-05-20 16:08:13,555 - plots:1415 - INFO - ROC Label digit_8 area: 0.997 n=467\n",
      "2020-05-20 16:08:13,559 - plots:1415 - INFO - ROC Label digit_9 area: 0.998 n=449\n",
      "2020-05-20 16:08:13,987 - plots:1430 - INFO - Saved ROC curve at: ./recipes_output/learn_mnist/per_class_roc_mnist_label.png\n",
      "2020-05-20 16:09:14,125 - plots:1701 - INFO - Saved T-SNE plot at: ./recipes_output/learn_mnist/tsne_plot.png\n",
      "2020-05-20 16:09:14,128 - tensor_generators:213 - INFO - Stopped 4 workers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'digit_0': 0.9994072829418441,\n",
       " 'digit_1': 0.9989927736924202,\n",
       " 'digit_2': 0.9969640627465777,\n",
       " 'digit_3': 0.997153939829099,\n",
       " 'digit_4': 0.9990085602118797,\n",
       " 'digit_5': 0.9991859351800887,\n",
       " 'digit_6': 0.9995755858579529,\n",
       " 'digit_7': 0.9966118795387089,\n",
       " 'digit_8': 0.9971490147570238,\n",
       " 'digit_9': 0.997507164981145}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', './mnist_hd5s/', \n",
    "            '--input_tensors', 'mnist_image',\n",
    "            '--output_tensors', 'mnist_label',\n",
    "            '--test_steps', '300',\n",
    "            '--epochs', '4',\n",
    "            '--id', 'learn_mnist'\n",
    "           ]\n",
    "args = parse_args()\n",
    "train_multimodal_multitask(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_mnist(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss:\n",
    "Our favorite loss function for categorical data.\n",
    "\\begin{equation}\n",
    "L(true, model) = -\\sum_{x\\in\\mathcal{X}} true(x)\\, \\log model(x)\n",
    "\\end{equation}\n",
    "\n",
    "Binary cross entropy with $N$ data points $x$ each with a binary label: \n",
    "\\begin{equation}\n",
    "true(x) \\in \\{0, 1\\} \\\\\n",
    "L(true, model) = -\\frac{1}{N}\\sum^N_{i=1} true(x_i)\\log(model(x_i)) + (1-true(x_i))log(1-model(x_i))\n",
    "\\end{equation}\n",
    "\n",
    "This is the Kullback Leibler divergence between the true distribution and the predicted. \n",
    "This function emerges in many fields as diverse as probability, information theory, and physics.\n",
    "What is the information difference between the truth and our model?  How much data do I lose by replacing the truth with the model's predictions. What is the temperature difference between my predictions and the truth?!\n",
    "\n",
    "Categorical cross entropy with $K$ different classes or labels: \n",
    "\\begin{equation}\n",
    "true(x) \\in \\{0, 1, 2, ..., K\\} \\\\\n",
    "L(true, model) = -\\frac{1}{N}\\sum^N_{i=1}\\sum^K_{j=1} y_{ik}\\log(q_k(x_i)))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Models: \"Hidden\" Layers and The MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron():\n",
    "    train, test, valid = load_data('mnist.pkl.gz')\n",
    "\n",
    "    num_labels = 10\n",
    "    train_y = make_one_hot(train[1], num_labels)\n",
    "    valid_y = make_one_hot(valid[1], num_labels)\n",
    "    test_y = make_one_hot(test[1], num_labels)\n",
    "\n",
    "    mlp_model = Sequential()\n",
    "    mlp_model.add(Dense(500, activation='relu', input_dim=784))\n",
    "    mlp_model.add(Dense(num_labels, activation='softmax'))\n",
    "    mlp_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    mlp_model.summary()\n",
    "    mlp_model.fit(train[0], train_y, validation_data=(valid[0],valid_y), batch_size=32, epochs=3)\n",
    "    print('Multilayer Perceptron trained. Test set loss and accuracy:', mlp_model.evaluate(test[0], test_y))\n",
    "\n",
    "multilayer_perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions Flip, Slide, Multiply, Add\n",
    "Convolutions look for their kernel in a larger signal.\n",
    "\n",
    "In convolution, you always and only find what you're looking with.\n",
    "\n",
    "Convolution and cross correlation are deeply related:\n",
    "\n",
    "\\begin{equation}\n",
    "f(t) \\circledast g(t) \\triangleq\\ \\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau) \\, d\\tau. = \\int_{-\\infty}^\\infty f(t-\\tau) g(\\tau)\\, d\\tau.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/2/21/Comparison_convolution_correlation.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_neural_network(filters=32, kernel_size=(3,3), padding='valid', num_labels = 10):\n",
    "    train, test, valid = load_data('mnist.pkl.gz')\n",
    "\n",
    "    train_y = make_one_hot(train[1], num_labels)\n",
    "    valid_y = make_one_hot(valid[1], num_labels)\n",
    "    test_y = make_one_hot(test[1], num_labels)\n",
    "    \n",
    "    print(train[0].shape)\n",
    "    mnist_images = train[0].reshape((-1, 28, 28, 1))\n",
    "    mnist_valid = valid[0].reshape((-1, 28, 28, 1))\n",
    "    mnist_test = test[0].reshape((-1, 28, 28, 1))\n",
    "\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv2D(input_shape=(28, 28, 1), filters=filters, kernel_size=kernel_size, padding=padding, activation='relu'))\n",
    "    cnn_model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='relu'))\n",
    "    cnn_model.add(Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, activation='relu'))\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(16, activation='relu'))\n",
    "    cnn_model.add(Dense(num_labels, activation='softmax'))\n",
    "    cnn_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    cnn_model.summary()\n",
    "    cnn_model.fit(mnist_images, train_y, validation_data=(mnist_valid, valid_y), batch_size=32, epochs=3)\n",
    "    \n",
    "    print('Convolutional Neural Network trained. Test set loss and accuracy:', cnn_model.evaluate(mnist_test, test_y))\n",
    "\n",
    "convolutional_neural_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why (and When!) is Convolution Helpful?\n",
    "- Decouples input size from model size\n",
    "- Translationally Equivariant (Not Invariant), so we can find features wherever they might occur in the signal\n",
    "- Local structure is often informative\n",
    "- But not always! (eg Tabular data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4CVD Cosmology:  Tensors all the way down.\n",
    "\n",
    "\n",
    "# ML4CVD Abstractions: Tensorization, the TensorMap, and the ModelFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorization\n",
    "Tensorization is the process of gathering any number of input files and consolidating them into compressed HD5 files.  We tend to make one HD5 file per sample in the study.  The files contain the raw data and labels we will use to train models.  It tends to be efficient to separate tensor construction from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dicoms(dicom_folder, stats):\n",
    "    dcm_file = ''\n",
    "    my_stats = Counter()\n",
    "    series = defaultdict(list)\n",
    "    for root, _, files in os.walk(dicom_folder):\n",
    "        for name in files:\n",
    "            dcm_file = os.path.join(root, name)\n",
    "            if not dcm_file.endswith('.dcm'):\n",
    "                continue\n",
    "            try:\n",
    "                dcm = pydicom.read_file(dcm_file)\n",
    "                my_stats[f'Shape {dcm.pixel_array.shape}'] += 1\n",
    "                my_stats['count'] += 1\n",
    "                my_stats[f'Series Number {dcm.SeriesNumber}'] += 1\n",
    "                my_stats[f'Series Description {dcm.SeriesDescription}'] += 1\n",
    "                my_stats[f'Pixel spacing{dcm.PixelSpacing}'] += 1\n",
    "                series[dcm.SeriesDescription.lower().trim().replace(' ', '_').replace('/', '_')].append(dcm)\n",
    "            except:\n",
    "                my_stats['got an error'] += 1\n",
    "                break\n",
    "    print(f'\\n At DICOM {dcm_file}')\n",
    "    for k in my_stats:\n",
    "        print(f'{k} has {my_stats[k]}') \n",
    "    \n",
    "    try:\n",
    "        tensors = {}\n",
    "        for k in series:\n",
    "            tensors[k] = np.zeros((512, 512, len(series[k])))\n",
    "            for dcm in series[k]:\n",
    "                tensors[k][..., dcm.InstanceNumber-1] = dcm.pixel_array\n",
    "    except:\n",
    "        my_stats['got a tensorization error'] += 1\n",
    "            \n",
    "    stats.update(my_stats)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_qure_ai_dicoms(zip_folder, dicom_folder, hd5_folder, delete_dicoms=True, limit_dicoms=-1):\n",
    "    raw_data = {}\n",
    "    stats = Counter()\n",
    "    if not os.path.exists(dicom_folder):\n",
    "        os.makedirs(dicom_folder)\n",
    "    if not os.path.exists(hd5_folder):\n",
    "        os.makedirs(hd5_folder)    \n",
    "    for z in os.listdir(zip_folder):\n",
    "        if os.path.exists(os.path.join(hd5_folder, z.replace('.zip', '.hd5'))):\n",
    "            continue\n",
    "        with zipfile.ZipFile(zip_folder + z, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(dicom_folder)\n",
    "            tensors = parse_dicoms(dicom_folder, stats)\n",
    "            with h5py.File(os.path.join(hd5_folder, z.replace('.zip', '.hd5')), 'w') as hd5:\n",
    "                for t in tensors:\n",
    "                    hd5.create_dataset(t, data=tensors[t], compression='gzip')\n",
    "        if delete_dicoms:\n",
    "            shutil.rmtree(dicom_folder)\n",
    "        if limit_dicoms > 0 and stats['count'] > limit_dicoms:\n",
    "            break\n",
    "\n",
    "    print('\\n\\n Full stats below:')\n",
    "    for k in stats:\n",
    "        print(f'{k} has {stats[k]}')\n",
    "\n",
    "tensorize_qure_ai_dicoms(ZIP_FOLDER, './dicoms/', './test/', limit_dicoms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 ./reads.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_qure_ai_reads(read_file, hd5_folder):\n",
    "    stats = Counter()\n",
    "    with open(read_file, 'r') as my_csv:\n",
    "        lol = list(csv.reader(my_csv, delimiter=','))\n",
    "        header = [h.replace(':', '_').lower() for h in lol[0]]\n",
    "        print(f'header is {header}')\n",
    "        for row in lol[1:]:\n",
    "            try:\n",
    "                with h5py.File(f'{hd5_folder}{row[0]}.hd5', 'a') as hd5:\n",
    "                    for i, value in enumerate(row[2:]):\n",
    "                        hd5.create_dataset(header[i+2], data=[int(value)])\n",
    "            except:\n",
    "                print(f'Could not open {row[0]}')\n",
    "    for k in stats:\n",
    "        print(f'{k} has {stats[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the DICOM series we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorMaps\n",
    "The critical data structure in the ML4CVD codebase is the TensorMap.\n",
    "This abstraction provides a way to translate ***any*** kind of input data, into structured numeric tensors with clear semantics for interpretation and modeling.  TensorMaps guarantee a shape, a way to consturct tensors of that shape from the HD5 files created during tensorization and a meaning to the values in the tensor that the TensorMap yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_image = TensorMap('mnist_image', Interpretation.CONTINUOUS, shape=(28, 28))\n",
    "mnist_image = TensorMap('mnist_image', Interpretation.CONTINUOUS, shape=(28, 28, 1))\n",
    "\n",
    "mnist_channel_map = {'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'zero': 4, 'zero': 5, \n",
    "                     'six': 6, 'seven': 9, 'eight': 8, 'nine': 9}\n",
    "\n",
    "mnist_class = TensorMap('mnist_class', Interpretation.CATEGORICAL, shape=(10,), channel_map=mnist_channel_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_image = TensorMap('cifar_image', Interpretation.CONTINUOUS, shape=(32, 32, 3))\n",
    "\n",
    "cifar_channel_map = {'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, \n",
    "                     'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
    "\n",
    "cifar_class = TensorMap('cifar_class', Interpretation.CATEGORICAL, shape=(10,), channel_map=cifar_channel_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmaps_by_sample_id(tensor_folder: str, sample_id: str, tmaps: List[TensorMap]):\n",
    "    path = os.path.join(tensor_folder, sample_id + '.hd5')\n",
    "    result_dict = defaultdict(lambda: None)\n",
    "    if os.path.isfile(path):\n",
    "        with h5py.File(path, 'r') as hd5:\n",
    "            for tmap in tmaps:\n",
    "                try:\n",
    "                    result_dict[tmap] = tmap.normalize_and_validate(tmap.tensor_from_file(tmap, hd5))\n",
    "                except (IndexError, KeyError, ValueError, OSError, RuntimeError):\n",
    "                    continue\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_array_3d(a):\n",
    "    slice_axis = -1\n",
    "    sides = int(np.ceil(np.sqrt(a.shape[slice_axis])))\n",
    "    _, axes = plt.subplots(sides, sides, figsize=(16, 16))\n",
    "    print(a.shape)\n",
    "    vmin = np.min(a)\n",
    "    vmax = np.max(a)\n",
    "    for i in range(a.shape[slice_axis]):\n",
    "        axes[i//sides, i%sides].imshow(a[..., i], cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        axes[i//sides, i%sides].set_yticklabels([])\n",
    "        axes[i//sides, i%sides].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_map(exclude=EXCLUDE_SERIES):\n",
    "    def slice_from_hd5(tm, hd5, dependents={}):\n",
    "        for k in np.random.permutation(list(hd5.keys())):\n",
    "            if any(x in k for x in exclude):\n",
    "                continue\n",
    "            if isinstance(hd5[k], h5py.Dataset) and len(hd5[k].shape) == 3 and hd5[k].shape[-1] > 50:\n",
    "                random_index = np.random.randint(hd5[k].shape[-1]-tm.shape[-1])\n",
    "                random_slices = np.array(hd5[k][..., random_index:random_index+tm.shape[-1]], dtype=np.float32)\n",
    "                return random_slices\n",
    "        raise ValueError('No CT slices in HD5')\n",
    "    return slice_from_hd5\n",
    "   \n",
    "TMAPS['slice_map'] = TensorMap('slice_map', shape=(512, 512, 1), tensor_from_file=slice_map(), \n",
    "                               cacheable=False, normalization={'zero_mean_std1': True})\n",
    "TMAPS['slice_map5'] = TensorMap('slice_map5', shape=(512, 512, 5), tensor_from_file=slice_map(), \n",
    "                                cacheable=False, normalization={'zero_mean_std1': True})\n",
    "TMAPS['slice_map25'] = TensorMap('slice_map25', shape=(512, 512, 25), tensor_from_file=slice_map(), \n",
    "                                 cacheable=False, normalization={'zero_mean_std1': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tmaps_by_sample_id(HD5_FOLDER, 'CQ500-CT-222', [ _get_tmap('slice_map25')])\n",
    "for k in t:\n",
    "    print(k.name, 'has', t[k].shape)\n",
    "    plot_array_3d(t[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_voxels(exclude=EXCLUDE_SERIES):\n",
    "    def crop_voxels_from_hd5(tm, hd5, dependents={}):\n",
    "        for k in np.random.permutation(list(hd5.keys())):\n",
    "            if isinstance(hd5[k], h5py.Dataset) and len(hd5[k].shape) == 3:\n",
    "                if any(x in k for x in exclude):\n",
    "                    continue\n",
    "                if hd5[k].shape[-1] < 50:\n",
    "                    continue\n",
    "                x_index = np.random.randint(hd5[k].shape[0]-tm.shape[0])\n",
    "                y_index = np.random.randint(hd5[k].shape[1]-tm.shape[1])\n",
    "                middle_index = hd5[k].shape[-1] // 2\n",
    "                start_index = middle_index - (tm.shape[-1] // 2)\n",
    "                stop_index = middle_index + (tm.shape[-1] // 2) + tm.shape[-1]%2\n",
    "                z_index = np.random.randint(hd5[k].shape[2]-tm.shape[2])\n",
    "                random_slices = np.array(hd5[k][x_index:x_index+tm.shape[0], y_index:y_index+tm.shape[1], start_index:stop_index], dtype=np.float32)\n",
    "                return random_slices\n",
    "        raise ValueError('No CT slices in HD5')\n",
    "    return crop_voxels_from_hd5\n",
    "\n",
    "TMAPS['crop_thin_16'] = TensorMap('crop_thin_16', shape=(16, 16, 16), tensor_from_file=crop_voxels(), \n",
    "                                  cacheable=False, normalization={'zero_mean_std1': True})\n",
    "    \n",
    "TMAPS['crop_thin_64'] = TensorMap('crop_thin_64', shape=(64, 64, 64), tensor_from_file=crop_voxels(), \n",
    "                                  cacheable=False, normalization={'zero_mean_std1': True})\n",
    "\n",
    "TMAPS['crop_thin_316'] = TensorMap('crop_thin_316', shape=(316, 192, 64), tensor_from_file=crop_voxels(), \n",
    "                                   cacheable=False, normalization={'zero_mean_std1': True})\n",
    "\n",
    "TMAPS['crop_thin_372'] = TensorMap('crop_thin_372', shape=(372, 292, 121), tensor_from_file=crop_voxels(), \n",
    "                                   cacheable=False, normalization={'zero_mean_std1': True})\n",
    "\n",
    "TMAPS['crop_thin_256'] = TensorMap('crop_thin_256', shape=(256, 256, 128), tensor_from_file=crop_voxels(), \n",
    "                                   cacheable=False, normalization={'zero_mean_std1': True})\n",
    "\n",
    "TMAPS['crop_thin_396'] = TensorMap('crop_thin_396', shape=(396, 396, 144), tensor_from_file=crop_voxels(), \n",
    "                                   cacheable=False, normalization={'zero_mean_std1': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tmaps_by_sample_id(HD5_FOLDER, 'CQ500-CT-212', [ _get_tmap('crop_thin_316')])\n",
    "for k in t:\n",
    "    print(k.name, 'has', t[k].shape)\n",
    "    plot_array_3d(t[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thin_slice(exclude=EXCLUDE_SERIES):\n",
    "    def thin_slice_from_hd5(tm, hd5, dependents={}):\n",
    "        for k in np.random.permutation(list(hd5.keys())):\n",
    "            if any(x in k for x in exclude):\n",
    "                continue\n",
    "            if hd5[k].shape[-1] < 50:\n",
    "                continue\n",
    "            if isinstance(hd5[k], h5py.Dataset) and len(hd5[k].shape) == 3:\n",
    "                random_index = np.random.randint(hd5[k].shape[-1]-tm.shape[-1])\n",
    "                random_slices = np.array(hd5[k][..., random_index:random_index+tm.shape[-1]], dtype=np.float32)\n",
    "                random_slices = np.clip(random_slices, 3000, 10000)\n",
    "                return random_slices\n",
    "        raise ValueError('No CT slices in HD5')\n",
    "    return thin_slice_from_hd5\n",
    "    \n",
    "TMAPS['thin_slice_map16'] = TensorMap('thin_slice_map16', shape=(512, 512, 16), tensor_from_file=thin_slice([]), \n",
    "                                      cacheable=False, normalization={'zero_mean_std1': True})\n",
    "TMAPS['thin_slice_map50'] = TensorMap('thin_slice_map50', shape=(512, 512, 50), tensor_from_file=thin_slice([]), \n",
    "                                      cacheable=False, normalization={'zero_mean_std1': True})\n",
    "TMAPS['thin_slice_map200'] = TensorMap('thin_slice_map200', shape=(512, 512, 200), tensor_from_file=thin_slice(), \n",
    "                                       cacheable=False, normalization={'zero_mean_std1': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tmaps_by_sample_id(HD5_FOLDER, 'CQ500-CT-212', [ _get_tmap('thin_slice_map50')])\n",
    "for k in t:\n",
    "    print(k.name, 'has', t[k].shape)\n",
    "    plot_array_3d(t[k])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with something simple and obvious for a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_count(exclude=EXCLUDE_SERIES):\n",
    "    def slice_count_hd5(tm, hd5, dependents={}):\n",
    "        for k in np.random.permutation(list(hd5.keys())):\n",
    "            if any(x in k for x in exclude):\n",
    "                continue\n",
    "            if isinstance(hd5[k], h5py.Dataset) and len(hd5[k].shape) == 3 and hd5[k].shape[-1] > 50:\n",
    "                random_index = np.random.randint(hd5[k].shape[-1]-tm.shape[-1])\n",
    "                random_slices = np.array(hd5[k][..., random_index:random_index+tm.shape[-1]], dtype=np.float32)\n",
    "                dependents[tm.dependent_map] = np.sum(random_slices > 800) / np.prod(random_slices.shape)\n",
    "                return random_slices\n",
    "        raise ValueError('No CT slices in HD5')\n",
    "    return slice_count_hd5\n",
    "\n",
    "TMAPS['slice_map_count'] = TensorMap('slice_map_count', shape=(1,), channel_map={'count': 0}, cacheable=False)\n",
    "TMAPS['slice_map_to_count'] = TensorMap('slice_map', shape=(512, 512, 2), dependent_map=TMAPS['slice_map_count'],\n",
    "                                        tensor_from_file=slice_count(), cacheable=False, \n",
    "                                        normalization={'zero_mean_std1': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMAPS['r1_fracture'] = TensorMap('r1_fracture', Interpretation.CATEGORICAL, \n",
    "                                 storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                 channel_map={'no_r1_fracture': 0, 'r1_fracture': 1})\n",
    "TMAPS['r2_fracture'] = TensorMap('r2_fracture', Interpretation.CATEGORICAL, \n",
    "                                 storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                 channel_map={'no_r2_fracture': 0, 'r2_fracture': 1})\n",
    "TMAPS['r3_fracture'] = TensorMap('r3_fracture', Interpretation.CATEGORICAL, \n",
    "                                 storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                 channel_map={'no_r3_fracture': 0, 'r3_fracture': 1})\n",
    "TMAPS['r1_calvarialfracture'] = TensorMap('r1_calvarialfracture', Interpretation.CATEGORICAL, \n",
    "                                          storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                          channel_map={'no_r1_calvarialfracture': 0, 'r1_calvarialfracture': 1})\n",
    "TMAPS['r2_calvarialfracture'] = TensorMap('r2_calvarialfracture', Interpretation.CATEGORICAL, \n",
    "                                          storage_type=StorageType.CATEGORICAL_INDEX, channel_map={'no_r2_calvarialfracture': 0, 'r2_calvarialfracture': 1})\n",
    "TMAPS['r3_calvarialfracture'] = TensorMap('r3_calvarialfracture', Interpretation.CATEGORICAL, \n",
    "                                          storage_type=StorageType.CATEGORICAL_INDEX, channel_map={'no_r3_calvarialfracture': 0, 'r3_calvarialfracture': 1})\n",
    "\n",
    "TMAPS['r1_masseffect'] = TensorMap('r1_masseffect', Interpretation.CATEGORICAL, \n",
    "                                   storage_type=StorageType.CATEGORICAL_INDEX, channel_map={'no_r1_masseffect': 0, 'r1_masseffect': 1})\n",
    "TMAPS['r2_masseffect'] = TensorMap('r2_masseffect', Interpretation.CATEGORICAL, \n",
    "                                   storage_type=StorageType.CATEGORICAL_INDEX, channel_map={'no_r2_masseffect': 0, 'r2_masseffect': 1})\n",
    "TMAPS['r3_masseffect'] = TensorMap('r3_masseffect', Interpretation.CATEGORICAL, \n",
    "                                   storage_type=StorageType.CATEGORICAL_INDEX, channel_map={'no_r3_masseffect': 0, 'r3_masseffect': 1})\n",
    "\n",
    "TMAPS['r1_ich'] = TensorMap('r1_ich', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r1_ich': 0, 'r1_ich': 1})\n",
    "TMAPS['r2_ich'] = TensorMap('r2_ich', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r2_ich': 0, 'r2_ich': 1})\n",
    "TMAPS['r3_ich'] = TensorMap('r3_ich', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r3_ich': 0, 'r3_ich': 1})\n",
    "\n",
    "TMAPS['r1_iph'] = TensorMap('r1_iph', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r1_iph': 0, 'r1_iph': 1})\n",
    "TMAPS['r2_iph'] = TensorMap('r2_iph', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r2_iph': 0, 'r2_iph': 1})\n",
    "TMAPS['r3_iph'] = TensorMap('r3_iph', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r3_iph': 0, 'r3_iph': 1})\n",
    "\n",
    "TMAPS['r1_midlineshift'] = TensorMap('r1_midlineshift', Interpretation.CATEGORICAL, \n",
    "                                     storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                     channel_map={'no_r1_midlineshift': 0, 'r1_midlineshift': 1})\n",
    "TMAPS['r2_midlineshift'] = TensorMap('r2_midlineshift', Interpretation.CATEGORICAL, \n",
    "                                     storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                     channel_map={'no_r2_midlineshift': 0, 'r2_midlineshift': 1})\n",
    "TMAPS['r3_midlineshift'] = TensorMap('r3_midlineshift', Interpretation.CATEGORICAL, \n",
    "                                     storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                     channel_map={'no_r3_midlineshift': 0, 'r3_midlineshift': 1})\n",
    "\n",
    "TMAPS['r1_sah'] = TensorMap('r1_sah', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r1_sah': 0, 'r1_sah': 1})\n",
    "TMAPS['r2_sah'] = TensorMap('r2_sah', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r2_sah': 0, 'r2_sah': 1})\n",
    "TMAPS['r3_sah'] = TensorMap('r3_sah', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r3_sah': 0, 'r3_sah': 1})\n",
    "\n",
    "TMAPS['r1_sdh'] = TensorMap('r1_sdh', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r1_sdh': 0, 'r1_sdh': 1})\n",
    "TMAPS['r2_sdh'] = TensorMap('r2_sdh', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r2_sdh': 0, 'r2_sdh': 1})\n",
    "TMAPS['r3_sdh'] = TensorMap('r3_sdh', Interpretation.CATEGORICAL, storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                            channel_map={'no_r3_sdh': 0, 'r3_sdh': 1})\n",
    "\n",
    "TMAPS['r1_chronicbleed'] = TensorMap('r1_chronicbleed', Interpretation.CATEGORICAL, \n",
    "                                     storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                     channel_map={'no_r1_chronicbleed': 0, 'r1_chronicbleed': 1})\n",
    "TMAPS['r2_chronicbleed'] = TensorMap('r2_chronicbleed', Interpretation.CATEGORICAL, \n",
    "                                     storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                     channel_map={'no_r2_chronicbleed': 0, 'r2_chronicbleed': 1})\n",
    "TMAPS['r3_chronicbleed'] = TensorMap('r3_chronicbleed', Interpretation.CATEGORICAL, \n",
    "                                     storage_type=StorageType.CATEGORICAL_INDEX, \n",
    "                                     channel_map={'no_r3_chronicbleed': 0, 'r3_chronicbleed': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model Factory\n",
    "The function ***make_multimodal_multitask_model()*** takes lists of TensorMaps and connects them with intelligent goo.\n",
    "### Model Architectures\n",
    "- Classification\n",
    "- Regression\n",
    "- Multitask\n",
    "- Multimodal\n",
    "- Multimodal Multitask\n",
    "- Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification of CT Slice Command Line & Architecture\n",
    "Jupyter is great, but can complicate productionizing code. We try to mitigate this by interacting with the jupyter notebook as if it were a command line call to one of ML4CVD's modes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'crop_thin_64',\n",
    "            '--output_tensors', 'r1_masseffect',\n",
    "            '--dense_blocks', '12', '12', '8', '8',\n",
    "            '--num_workers', '0', '--cache_size', '0',\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "_inspect_model(model, generate_train, generate_valid, 1, 1, True, './my_first_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![My First Model Architecture](./my_first_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'slice_map_to_count',\n",
    "            '--output_tensors', 'slice_map_count',\n",
    "            '--pool_x', '6', '--pool_y', '6',\n",
    "            '--id', 'slice_map_to_count',\n",
    "            '--output_folder', MODEL_FOLDER,\n",
    "            '--epochs', '2',\n",
    "            '--training_steps', '48', \n",
    "            '--validation_steps', '22', \n",
    "            '--valid_ratio', '0.3', \n",
    "            '--test_steps', '48', \n",
    "            '--test_ratio', '0.3', \n",
    "            '--test_modulo', '0', \n",
    "            '--batch_size', '4',\n",
    "            '--cache_size', '0',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "train_multimodal_multitask(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['test_scalar', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'slice_map_to_count',\n",
    "            '--output_tensors', 'slice_map_count', 'r1_midlineshift', 'r2_midlineshift', 'r3_midlineshift',\n",
    "            '--id', 'slice_map_to_count',\n",
    "            '--output_folder', MODEL_FOLDER,\n",
    "            '--model_file', f'{MODEL_FOLDER}/slice_map_to_count/slice_map_to_count.hd5',\n",
    "            '--epochs', '18',\n",
    "            '--training_steps', '48', \n",
    "            '--validation_steps', '22', \n",
    "            '--valid_ratio', '0.3', \n",
    "            '--test_steps', '48', \n",
    "            '--test_ratio', '0.3', \n",
    "            '--test_modulo', '0', \n",
    "            '--batch_size', '4',\n",
    "            '--cache_size', '0',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "test_multimodal_multitask(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'crop_thin_64',\n",
    "            '--output_tensors', 'r1_midlineshift', 'r2_midlineshift', 'r3_midlineshift',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "_inspect_model(model, generate_train, generate_valid, 1, 1, True, './my_first_multitask_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multitask](./my_first_multitask_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'crop_thin_16', 'crop_thin_64',\n",
    "            '--output_tensors', 'r1_midlineshift',\n",
    "            '--dense_blocks', '12', '12', '8', '8',\n",
    "            '--test_ratio', '0.3',\n",
    "            '--num_workers', '0',\n",
    "            '--cache_size', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "_inspect_model(model, generate_train, generate_valid, 1, 1, True, './my_first_multimodal_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multimodal](./my_first_multimodal_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT Slice AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'crop_thin_64',\n",
    "            '--output_tensors', 'crop_thin_64',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "_inspect_model(model, generate_train, generate_valid, 1, 1, True, './my_ct_slice_autoencoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ct auto encoder](./my_ct_slice_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'crop_thin_64',\n",
    "            '--output_tensors', 'crop_thin_64',\n",
    "            '--u_connect',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "_inspect_model(model, generate_train, generate_valid, 1, 1, True, './my_ct_slice_unet_autoencoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unet](./my_ct_slice_unet_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware of the Irony of Command Line Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'crop_thin_64',\n",
    "            '--output_tensors', 'r1_calvarialfracture', 'r1_masseffect',\n",
    "            '--output_folder', MODEL_FOLDER,\n",
    "            '--activation', 'prelu',\n",
    "            '--dense_blocks', '32', '24', '16', '12', '8', '6',\n",
    "            '--dense_layers', '32', '32', '32',\n",
    "            '--block_size', '5',\n",
    "            '--pool_x', '2',\n",
    "            '--pool_y', '2',\n",
    "            '--id', 'middle_slices_16_r1_calvarialfracture_mass_effect',\n",
    "            '--epochs', '18',\n",
    "            '--training_steps', '48', \n",
    "            '--validation_steps', '22', \n",
    "            '--valid_ratio', '0.3', \n",
    "            '--test_steps', '2', \n",
    "            '--test_ratio', '0.1', \n",
    "            '--test_modulo', '0', \n",
    "            '--batch_size', '6',\n",
    "            '--cache_size', '0',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "_inspect_model(model, generate_train, generate_valid, 1, 1, True, './my_hypertune.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hyper](./my_hypertune.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['plot_saliency', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'slice_map_to_count',\n",
    "            '--output_tensors', 'slice_map_count',\n",
    "            '--id', 'slice_map_to_count',\n",
    "            '--output_folder', MODEL_FOLDER,\n",
    "            '--model_file', f'{MODEL_FOLDER}/slice_map_to_count/slice_map_to_count.hd5',\n",
    "            '--epochs', '18',\n",
    "            '--training_steps', '48', \n",
    "            '--validation_steps', '22', \n",
    "            '--valid_ratio', '0.3', \n",
    "            '--test_steps', '4', \n",
    "            '--test_ratio', '0.3', \n",
    "            '--test_modulo', '0', \n",
    "            '--batch_size', '4',\n",
    "            '--cache_size', '0',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "generate_train, generate_valid, generate_test = test_train_valid_tensor_generators(**args.__dict__)\n",
    "model = make_multimodal_multitask_model(**args.__dict__)\n",
    "saliency_maps(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE of learned Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![t-SNE](./models/slice_map_to_count/tsne_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['train', \n",
    "            '--tensors', HD5_FOLDER, \n",
    "            '--input_tensors', 'crop_thin_64',\n",
    "            '--output_tensors', 'r1_calvarialfracture', 'r1_masseffect',\n",
    "            '--output_folder', MODEL_FOLDER,\n",
    "            '--dense_blocks', '32', '24',\n",
    "            '--pool_x', '4',\n",
    "            '--pool_y', '4',\n",
    "            '--id', 'middle_slices_16_r1_calvarialfracture_mass_effect',\n",
    "            '--epochs', '18',\n",
    "            '--training_steps', '48', \n",
    "            '--validation_steps', '22', \n",
    "            '--valid_ratio', '0.3', \n",
    "            '--test_steps', '2', \n",
    "            '--test_ratio', '0.1', \n",
    "            '--test_modulo', '0', \n",
    "            '--batch_size', '6',\n",
    "            '--cache_size', '0',\n",
    "            '--num_workers', '0'\n",
    "           ]\n",
    "args = parse_args()\n",
    "train_multimodal_multitask(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
